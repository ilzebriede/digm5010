Cariani, Peter Anthony. On the design of devices with emergent semantic functions. Diss. State University of New York, 1989.
Chapter 12 Emergence and open-endedness (148-169).
>Emergence is directly connected with the notion of novelty. Cariani states that "If we want our devices to be creative in any meaningful sense of the word, they must be capable of emergent behaviour if implementing functions we have not specified." (p.148) Cariani lists three types of emergences: physical (water molecule states as ice, water, steam = thermodynamic model, order from noise), biological (such as life = form from formlessness, order from chaos), and psychological (aha moment). There is also a computational emergence, such as a-life simulations and cellular automata. Another point Cariani is trying to get across is that the "higher-level patterns must be recognised by the human observer. [...] it is a mistake to believe this to be a property of the device." (p.152) I would argue here that we are not trying to attribute who or what owns the emergent pattern. In my opinion, the key is that there is a moment of acknowledgement that such pattern even exists. I am curious about how can we come to this moment of realisation, that the emergent behaviour or pattern has happened. I don't think it is about why and how, it is about a moment of encounter, realisation. Even though this moment of discovery does flash up within our human psyche, something external initiated it. We need to look at how to expand this space of the encounter into a field of enquiring, a space for work and experiments.
Cariani says that "emergence relative to a model (biologically based emergence - I.B.), then is a result of the finite and hence incomplete character of all models of the world." (p.157) So be it! Why can't we operate on incomplete models? For Nietzsche, language and words are moving away from the truth. Nevertheless, we do use language as a means of communication, no matter how far we are from the true essence of the truthfulness this word abstracts from. It is our only way to communicate via spoken and written language. It is complete in its incompleteness.
>Emergent system 1)ability to measure and affect changes in outside world 2)capacity for adaptive self-alteration. Cariani calls them "syntactically adaptive" = genetic algorithms, "semantically adaptive" = this mapping interprets the input = forming a new sensory organ. But is this a "new" organ, or is it same organ but in a different configuration?

Sommerer, Christa, and Laurent Mignonneau. "Modeling the emergence of complexity: Complex systems, the origin of life and interactive on-line art." Leonardo 35.2 (2002): 161-169.
> Keywords: *Complex systems theory, emergence, A- Life, edge-of-chaos (C.Langton, N.Packard)*
This academic research paper describes an Internet-based interactive artwork called VERBARIUM (1999) created in order to model and simulate creative emergence based on participator engagement and interactions. The principle discussed for catalising interconnectedness and collective co-creating are phase transitions (Complex systems theory); a balance act and the midpoint between order and disorder, stability and flexibility, intent and meaning. In VERBARIUM an input digital text gets transformed into 3D form, then added and stored with a collective image transforming and evolving overtime. The process is supported by specially designed text-to-form editor that morphs and growcs 8 vertices ring into complex growing structure.

Whitelaw, Mitchell. Metacreation: art and artificial life. Mit Press, 2006. Chapter: Emergence (pp.207-237)
> Whitelaw distinguishes two types of levels in a-life art emergence: *local* and *global*. If local emergence is informed by a computational set of rules and functions, then *global* level emergence appears as behavioural "patterns in time or space" (ibid. 214). This type of emergence becomes a new phenomenon that wasn't programmed beforehand. The local level means "technological substrate", and it is software and hardware. The global level means "phenomenal and behavioural product of that technological substrate" (ibid., p.215). 
"In a-life systems there is no simple correspondence between substrate and phenomenon but a complex entangled causality giving rise to artifacts and events that seem to constitute something new, something extra" (ibid., p.215)
Whitelaw talks about a-life that "becomes other", in which case it needs to "surrender its intentionality at some point in this process" (p.226). My thoughts are that we can't speak about the intentionality, that implies that we do already understand the system to has some sort of intentionality. The question is: is this intentionality read by us, meaning, what we consider intentional, or is it really intended by system. But how can we know that?
"The evolved form can be considered an emergent phenomenon, one that has somehow exceeded or pulled from its mechanistic substrate." (ibid., p.215)
"Any system capable of autonomous ongoing emergence could move outside the bounds of its host system, across domains." (p.228)
"There is no reason why it (a-life art - I.B.) should stay in the gallery or in the computer. [...] It would be unbounded and unintentional, an adaptive pattern indistinguishable from the wider dynamics of its environment." (p.228)
Personal Note: This analogy reminds me of Walter Fontana's description of genotype being a system and configuration, and the genotype being the form and behaviour. One can't survive without the other. Same in here: the emergent behaviour within the computational world can't happen without the hardware and syntax of the code, but the code and hardware also inform such behaviour that exists within this type of system. Once again, I do think of the importance of the feedback loop and genetic systems where output can inform and alter the input, therefore becoming a self-sustaining self-"aware" system. The level and speed of calculations and tasks have also informed technology itself by becoming more accomodating and suitable for computation needs. However, I would argue that this is a human-created necessity to build more sophisticated machines to do more complex computations. The neural networks that reconstruct themselves to become more efficient at tasks are closer to self-sustaining, self-improving systems, but that only exists as software and not hardware.

Licklider, Joseph CR. "Man-computer symbiosis." Multimedia: From Wagner to virtual reality (2001): 55-63.
> The idea about computer-man symbiotic work practice where a machine takes over technical tasks while the human mind can engage more freely with the generation of ideas. A need to create a methodology where man and machine meet in productive collaborative space, able to communicate back and forth each other findings and needs. Dissimilarity and potential supplementation between man and machinic thinking. Challenges: language to aid two-way translation between parties,
"To bring computing machines effectively into processes of thinking that must go on in "real time," time that moves too fast to permit using computers in conventional ways." 
"If computer thinking could be introduced effectively into the thought process, the functions that can be performed by data-processing machines would improve or facilitate thinking and problem solving in an important way."
My personal thoughts: This article was originally written in 1960, and it paints the future we already reached. The question about human-computer symbiosis is still relevant, and it hasn't been answered in the past 60 years. The computers have evolved as we have developed ways to interact with them. However, computers and computational programs are still solving problems. Symbiotic relationships can harness different interaction outcomes, and there are classified eight such interaction modules. What Licklider was suggesting by offering a fig tree and *Blastonphaga grossorum* insect example, is a mutualistic relationship where both agencies benefit from each other. It seems to me that only humans have profited from computers, and it is hard to talk about mutual inter-relationship as computers are not independent agents, but programmed and constructed by humans. Additionally, Lickider suggests that "computer [...] will accept a clearly secondary status" in decision making and recognising relevance. If this type of relationship is formed where human will take an upper-hand, doesn't it question and distrusts the computational process to be equally relevant and important? Machine learning is operating on given data that becomes it's learned memory. Even though neural networks of computerised systems are built inspired by brain neural networks, we are still learning to understand how decisions are formed in unsupervised computerised systems. We have created something that we do not understand ourselves, and that has become a subject of inquiry in itself. We feed computers data that represents humans in the hope that they will form a sort of essence or ways to reflect on our ways or show something that we didn't know. If we feed computer algorithms known and existing data, how can they create something novel? Aren't we making an echo-chamber of our world, transmitted, recycled and rebuilt by machine? How can this data exchange benefit us and computers in general? 
Symbiosis is described as "cooperative "living together" in intimate association, or even close union, of two dissimilar organisms" ("Welister's New International Dictionary," 2n(l e(i., G. and C.Mlerriam Co., Springfield, Mass., p. 2555; 1958.) The keyword here is *intimate*, suggesting a close, personal and mutually understanding connection and exchange. Is it possible to build an intimate and close inter-personal relation between human and machine?

Ascott, Roy. Telematic embrace: Visionary theories of art, technology, and consciousness. Univ of California Press, 2007. Chapter: The Construction of Change (pp.97 - pp.108)
> For Roy Ascott, artwork occupies a space between two sets of behaviours: the artists and the spectators. "It is essentially a matrix, the substance *between*. It exists neither for itself nor by itself." (p.99) His *Change Paintings* are analogues of ideas, open to different interpretations and able of various states. The process that is an act of creation itself happens amid the artwork and the perceiver. I think this is Roy Ascott's "fluid field" that encourages, catalyses and supports the change. The journey from idea to its manifestation is more critical than the outcome, as each time the result can be ultimately a different form. The idea behind his Groundcourse (a two-year art program) is to allow creative mind seek possible outcomes to impossible things, an invitation to work with something unthinkable and unique and translate it via real-world materials and tools, and techniques. Because "out of the flux, a many-sided organism may evolve" (pp.102). He sees cybernetics as a unique angle to explore the environment, its structure, behaviour and building blocks, a methodology that artists can apply and expand through the process of art-making.  

Fontana, Walter. "The topology of the possible." Understanding Change. Palgrave Macmillan, London, 2006. 67-84.
>Keywords: *System = sequence, configuration, genotype. Behaviour = shape a.k.a. folded sequence, phenotype. Change, evolution.*
> In this draft paper, Fontana talks about how change happens in evolutionary systems such as RNA based on two main biological components: a phenotype and genotype. Suppose the phenotype is in charge of the system's behaviour. In that case, the genotype is a "heritable repository of information that participates in the production of molecules whose interactions, in conjunction with the environment, generate and maintain the phenotype." (ibid. p.2) The key phrase here is "environment", Fontana argues that environment alone is not what drives evolutionary change, but internal workings of the system itself. Fontana proposes looking at phenotype as a self-maintaining system that is a functional closure with intricate inner workings between its components. The feedback loop within the system ensures a state of homeostasis, or at least it is drifting towards that direction. It also allows any repairs of missing components, and it creates a certain level of robustness to the system. Fontana states that "robustness enables change". It is an exciting standpoint thinking about our evolutionary traits, proposing that stability and stasis are a catalyst for new formations, opposite to what history suggests in world events. Based on Fontana, change is bound to happen if the system, whether it is a human race, nature, or any other species, is fit and ready for it internally. When two different systems interact, the connection will foster a new type of components that neither belongs to the one or the other system. These new elements will serve as a "glue" allowing a cross-communication between these two autonomous systems. The change can't be propelled from outside. Once the inner structure and components reconfigure, it no longer is the same system. The change is a cause and effect in the worldbuilding. "The mechanisms of development are themselves subject to evolution, creating feedback between evolution and development." (ibid., p.5)

McLean, Alex, et al. "Visualisation of live code." Electronic Visualisation and the Arts (EVA 2010) (2010): 26-30.
> This article focuses on coding as live performance and lists several live coding languages and environments. many of them are created within Fluxus, a game engine meant for live performances and experiments. "Live coding has the unique opportunity to visualise the movement of an underlying process while it is being performed." (p.28) This article also talks about the body of these live coding environments, a collection of symbols interpreted by both, a coder and computer. These non-language visual environments offer a new communication method between coder and code interpreter, such as a computer. I also think these esoteric languages become so visually engaging because, in live coding performance, the coder screen is shared with the audience. This functional visualisation becomes a map of clues for the audience and map of instruments for the coder. The main underlying question in this paper is this: "Can a live coder elucidate the more abstract thinking gestures of their practice?" (p.26) To me, this topic is significant, as I have been live performing visuals since 2006. I have many VJing tools and appropriated other types of software for live performances, such as TouchDesigner, Photoshop with graphics tablet and pen, and Max/MSP. The choice of these visual instruments would be based on the visual style I could achieve. I would also experiment with data input and manipulation in real-time using midi controllers and sensor-based hardware. However, my working screen would be exclusively accessible to me, and the audience would see curated output minus the control panels. Live coding environments become input area and output screen simultaneously. It is a radically different approach to invite spectator inside the studio space to observe work being made. 

Griffiths, David, and Alex McLean. "Textility of Code: A Catalogue of Errors." TEXTILE 15.2 (2017): 198-214.
> This research article describes attempts to understand and communicate how weavers think and create textiles. Two programmers and researchers David Griffiths and Alex McLean from England approached this question from programming an coding. They both worked on creating computing algorithms using *Scheme* and *Haskell* programming languages to "help communicate the complexity of its (weaving - I.B.) structure and perhaps look for ways in which textiles could inform the development of computer science." (p.2) What was interesting in this article was the approach in thinking from the perspective of a thread. When translating weaving functions within the computing realm, there can be many departures or centres of dwelling, such as the perspective of a weaver, loom, or thread. It made me think about the programming world's multidimensionality, such as the egocentric or the allocentric approach. Where code can be written in parallel exiting from the object's perspective and then being translated into coordinates of the field this object is also a part. This article is dealing with the same questions I had ever since thinking about weaving patterns and computation. It even goes beyond creating computer programs that simulate different types of weaving, such as plain weave, four-shaft loom, tablet weaving, warp-weighted loom, all of those being ancient technologies developed and practised before the advent of computers. The key points I want to take from this article are:
>**Application of L-Systems in the generation of complex patterns.
>**The properties of thread, weave, and textile is based on rules, binary nature and three-dimensional phenomena.
>**Finally, the aspect of weavers response in real-time altering the weave corresponds to the idea of live coding.


